{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic[flair,gensim,spacy,use]\n",
      "  Using cached bertopic-0.16.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from bertopic[flair,gensim,spacy,use]) (1.24.3)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached hdbscan-0.8.37-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting umap-learn>=0.5.0 (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pandas>=1.1.5 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from bertopic[flair,gensim,spacy,use]) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from bertopic[flair,gensim,spacy,use]) (1.4.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from bertopic[flair,gensim,spacy,use]) (4.66.4)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting plotly>=4.7.0 (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached plotly-5.22.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tensorflow in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from bertopic[flair,gensim,spacy,use]) (2.10.0)\n",
      "Collecting tensorflow-hub (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tensorflow-text (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached tensorflow_text-2.10.0-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting transformers>=3.5.1 (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from bertopic[flair,gensim,spacy,use]) (2.3.1)\n",
      "Collecting flair>=0.7 (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached flair-0.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: gensim>=4.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from bertopic[flair,gensim,spacy,use]) (4.3.2)\n",
      "Collecting spacy>=3.0.1 (from bertopic[flair,gensim,spacy,use])\n",
      "  Using cached spacy-3.7.5-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting boto3>=1.20.27 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached boto3-1.34.144-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting bpemb>=0.3.2 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached bpemb-0.3.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting conllu>=4.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached conllu-5.0.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting gdown>=4.4.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting huggingface-hub>=0.10.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: janome>=0.4.2 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (0.5.0)\n",
      "Collecting langdetect>=1.0.9 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: lxml>=4.8.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (5.2.2)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (3.8.4)\n",
      "Requirement already satisfied: more-itertools>=8.13.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (10.3.0)\n",
      "Collecting mpld3>=0.3 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pptree>=3.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (2.9.0)\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (2023.10.3)\n",
      "Requirement already satisfied: segtok>=1.5.11 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (1.5.11)\n",
      "Requirement already satisfied: sqlitedict>=2.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (2.1.0)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (0.9.0)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (1.26.19)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: semver<4.0.0,>=3.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use]) (3.0.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from gensim>=4.0.0->bertopic[flair,gensim,spacy,use]) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from gensim>=4.0.0->bertopic[flair,gensim,spacy,use]) (5.2.1)\n",
      "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: joblib>=1.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from hdbscan>=0.8.29->bertopic[flair,gensim,spacy,use]) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from pandas>=1.1.5->bertopic[flair,gensim,spacy,use]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from pandas>=1.1.5->bertopic[flair,gensim,spacy,use]) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from plotly>=4.7.0->bertopic[flair,gensim,spacy,use]) (8.5.0)\n",
      "Requirement already satisfied: packaging in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from plotly>=4.7.0->bertopic[flair,gensim,spacy,use]) (23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic[flair,gensim,spacy,use]) (2.2.0)\n",
      "Requirement already satisfied: Pillow in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[flair,gensim,spacy,use]) (10.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (2.0.8)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached thinc-8.2.5-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (1.1.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (2.32.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Requirement already satisfied: jinja2 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (69.5.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use]) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use]) (4.11.0)\n",
      "Requirement already satisfied: sympy in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use]) (1.12)\n",
      "Requirement already satisfied: networkx in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use]) (3.2.1)\n",
      "Collecting fsspec (from torch>=1.4.0->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use]) (2021.4.0)\n",
      "Requirement already satisfied: colorama in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tqdm>=4.41.1->bertopic[flair,gensim,spacy,use]) (0.4.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from transformers>=3.5.1->bertopic[flair,gensim,spacy,use]) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from transformers>=3.5.1->bertopic[flair,gensim,spacy,use]) (0.4.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=3.5.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached numba-0.60.0-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (3.19.6)\n",
      "Requirement already satisfied: six>=1.12.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (1.42.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow->bertopic[flair,gensim,spacy,use]) (2.10.0)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorflow-hub->bertopic[flair,gensim,spacy,use]) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->bertopic[flair,gensim,spacy,use]) (0.43.0)\n",
      "Collecting botocore<1.35.0,>=1.34.144 (from boto3>=1.20.27->flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached botocore-1.34.144-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from boto3>=1.20.27->flair>=0.7->bertopic[flair,gensim,spacy,use]) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: sentencepiece in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from bpemb>=0.3.2->flair>=0.7->bertopic[flair,gensim,spacy,use]) (0.2.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from ftfy>=6.1.0->flair>=0.7->bertopic[flair,gensim,spacy,use]) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from gdown>=4.4.0->flair>=0.7->bertopic[flair,gensim,spacy,use]) (4.12.3)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use]) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use]) (3.0.9)\n",
      "Requirement already satisfied: intel-openmp==2021.* in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.4.0->bertopic[flair,gensim,spacy,use]) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.4.0->bertopic[flair,gensim,spacy,use]) (2021.13.0)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.2->umap-learn>=0.5.0->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached llvmlite-0.43.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (2024.7.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (3.0.3)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (1.5.4)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from jinja2->spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from sympy->torch>=1.4.0->bertopic[flair,gensim,spacy,use]) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (2.0.0)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached marisa_trie-1.2.0-cp310-cp310-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use]) (2.18.0)\n",
      "Collecting accelerate>=0.21.0 (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from beautifulsoup4->gdown>=4.4.0->flair>=0.7->bertopic[flair,gensim,spacy,use]) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair>=0.7->bertopic[flair,gensim,spacy,use]) (1.7.1)\n",
      "Requirement already satisfied: psutil in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from accelerate>=0.21.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use]) (5.9.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use])\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in t:\\miniconda3\\envs\\wassup_pr1\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->bertopic[flair,gensim,spacy,use]) (3.2.2)\n",
      "Using cached flair-0.13.1-py3-none-any.whl (388 kB)\n",
      "Using cached hdbscan-0.8.37-cp310-cp310-win_amd64.whl (602 kB)\n",
      "Using cached plotly-5.22.0-py3-none-any.whl (16.4 MB)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Using cached spacy-3.7.5-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "Using cached transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "Using cached umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
      "Using cached bertopic-0.16.2-py2.py3-none-any.whl (158 kB)\n",
      "Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Using cached tensorflow_text-2.10.0-cp310-cp310-win_amd64.whl (5.0 MB)\n",
      "Using cached boto3-1.34.144-py3-none-any.whl (139 kB)\n",
      "Using cached bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached conllu-5.0.1-py3-none-any.whl (16 kB)\n",
      "Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Using cached mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "Using cached numba-0.60.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Using cached preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Using cached srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "Using cached thinc-8.2.5-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Using cached transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "Using cached botocore-1.34.144-py3-none-any.whl (12.4 MB)\n",
      "Using cached cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached llvmlite-0.43.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Using cached accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "Using cached marisa_trie-1.2.0-cp310-cp310-win_amd64.whl (152 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: tensorflow-hub, preshed, plotly, mdurl, marisa-trie, llvmlite, langdetect, ftfy, fsspec, deprecated, cython, conllu, cloudpathlib, catalogue, blis, annotated-types, wikipedia-api, srsly, pydantic, numba, markdown-it-py, language-data, huggingface-hub, botocore, tokenizers, s3transfer, rich, pytorch-revgrad, pynndescent, mpld3, langcodes, hdbscan, gdown, confection, bpemb, accelerate, umap-learn, typer, transformers, thinc, boto3, weasel, sentence-transformers, transformer-smaller-training-vocab, tensorflow-text, spacy, bertopic, flair\n",
      "Successfully installed accelerate-0.32.1 annotated-types-0.7.0 bertopic-0.16.2 blis-0.7.11 boto3-1.34.144 botocore-1.34.144 bpemb-0.3.5 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 conllu-5.0.1 cython-0.29.37 deprecated-1.2.14 flair-0.13.1 fsspec-2024.6.1 ftfy-6.2.0 gdown-5.2.0 hdbscan-0.8.37 huggingface-hub-0.23.4 langcodes-3.4.0 langdetect-1.0.9 language-data-1.2.0 llvmlite-0.43.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 mpld3-0.5.10 numba-0.60.0 plotly-5.22.0 preshed-3.0.9 pydantic-2.8.2 pynndescent-0.5.13 pytorch-revgrad-0.2.0 rich-13.7.1 s3transfer-0.10.2 sentence-transformers-3.0.1 spacy-3.7.5 srsly-2.4.8 tensorflow-hub-0.16.1 tensorflow-text-2.10.0 thinc-8.2.5 tokenizers-0.19.1 transformer-smaller-training-vocab-0.4.0 transformers-4.42.4 typer-0.12.3 umap-learn-0.5.6 weasel-0.4.1 wikipedia-api-0.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script ftfy.exe is installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script gdown.exe is installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts accelerate-config.exe, accelerate-estimate-memory.exe, accelerate-launch.exe, accelerate-merge-weights.exe and accelerate.exe are installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script typer.exe is installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script weasel.exe is installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\Post Scriptum\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "# !pip install bertopic[flair,gensim,spacy,use] --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(path+\"debate_combined_ver2.csv\", index_col=0)\n",
    "df_2024 = pd.read_csv(path+\"2024_combined.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic은 트랜스포머에서 문맥정보 및 어텐션을 위해 불용어 처리를 안하는 것이 더 좋으므로\n",
    "from bertopic import BERTopic\n",
    "import ast\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "def do_bertopic(df):\n",
    "  vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "  representation_model = KeyBERTInspired()\n",
    "  topic_model = BERTopic(n_gram_range=(1,2), representation_model=representation_model, vectorizer_model=vectorizer_model)\n",
    "\n",
    "  topics, probs = topic_model.fit_transform(df[\"speech_whole\"])\n",
    "\n",
    "  return topic_model, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filter_externals(string):\n",
    "    string = re.sub('\\s*\\[[^]]*\\]', '', string)\n",
    "    string = re.sub('\\s*\\([^)]*\\)', '', string)\n",
    "    string = re.sub('\\s*\\([^]]*\\]', '', string)\n",
    "    string = re.sub('\\s*\\[[^)]*\\)', '', string)\n",
    "    return string\n",
    "\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "representation_model = KeyBERTInspired()\n",
    "topic_model = BERTopic(n_gram_range=(1,2), representation_model=representation_model, vectorizer_model=vectorizer_model)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(df_all[\"speech_whole\"].apply(filter_externals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1173</td>\n",
       "      <td>-1_the_to_and_of</td>\n",
       "      <td>[the, to, and, of, in, that, we, you, for, is]</td>\n",
       "      <td>[Thank you very much for being here and introd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>0_the_in_applause_of</td>\n",
       "      <td>[the, in, applause, of, is, and, that, we, thi...</td>\n",
       "      <td>[Senator KENNEDY. Lieutenant Governor Swainson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>1_to_you_and_we</td>\n",
       "      <td>[to, you, and, we, the, in, that, for, of, have]</td>\n",
       "      <td>[The President. Thank you. Audience members. F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>95</td>\n",
       "      <td>2_the_of_our_in</td>\n",
       "      <td>[the, of, our, in, and, to, that, we, is, as]</td>\n",
       "      <td>[For the past seventeen months, as a candidate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>3_the_to_and_our</td>\n",
       "      <td>[the, to, and, our, for, he, in, of, is, we]</td>\n",
       "      <td>[The President. Thank you all for coming today...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>4_you_clinton_to_that</td>\n",
       "      <td>[you, clinton, to, that, and, the, know, mccai...</td>\n",
       "      <td>[CLINTON: Hello, Unity! Hello, New Hampshire! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>5_of_the_farm_in</td>\n",
       "      <td>[of, the, farm, in, and, to, that, is, it, far...</td>\n",
       "      <td>[Senator KENNEDY. My friend and colleague, Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>6_and_our_the_to</td>\n",
       "      <td>[and, our, the, to, of, we, in, iraq, that, war]</td>\n",
       "      <td>[Thank you, Commander Kurpius, for that introd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>7_of_the_in_that</td>\n",
       "      <td>[of, the, in, that, to, and, would, have, is, be]</td>\n",
       "      <td>[The ceremony of an acceptance speech is a tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>8_iraq_that_the_to</td>\n",
       "      <td>[iraq, that, the, to, mccain, troops, and, we,...</td>\n",
       "      <td>[A few months ago, I met a woman who told me h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>55</td>\n",
       "      <td>9_the_texas_in_and</td>\n",
       "      <td>[the, texas, in, and, of, you, that, to, have,...</td>\n",
       "      <td>[Senator Tower, Congressman Bush, all of the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>10_crisis_financial_that_and</td>\n",
       "      <td>[crisis, financial, that, and, street, mccain,...</td>\n",
       "      <td>[We meet here at a time of great uncertainty f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>55</td>\n",
       "      <td>11_that_the_in_of</td>\n",
       "      <td>[that, the, in, of, we, is, this, and, it, to]</td>\n",
       "      <td>[Gov. JOHN F. DAVIS. North Dakota is again hon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "      <td>12_the_and_carolina_to</td>\n",
       "      <td>[the, and, carolina, to, of, in, that, you, we...</td>\n",
       "      <td>[I realize that in this great audience are man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>52</td>\n",
       "      <td>13_and_to_jobs_will</td>\n",
       "      <td>[and, to, jobs, will, the, that, of, tax, we, ...</td>\n",
       "      <td>[We meet at a moment of great uncertainty for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>14_energy_oil_of_and</td>\n",
       "      <td>[energy, oil, of, and, the, our, we, to, that,...</td>\n",
       "      <td>[Two weeks ago, representatives from some of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>15_mccain_obama_john_economic</td>\n",
       "      <td>[mccain, obama, john, economic, jobs, senator,...</td>\n",
       "      <td>[NORRIS: Senator Barack Obama was in St. Louis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>37</td>\n",
       "      <td>16_to_and_you_we</td>\n",
       "      <td>[to, and, you, we, michigan, the, in, for, tha...</td>\n",
       "      <td>[Hello Detroit! Thank you! I'll tell you, I am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>17_to_and_the_will</td>\n",
       "      <td>[to, and, the, will, tax, we, for, in, going, of]</td>\n",
       "      <td>[Thank you Arnold! Thank you Ohio! We need to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>18_to_the_you_and</td>\n",
       "      <td>[to, the, you, and, in, of, is, for, we, that]</td>\n",
       "      <td>[[1.] BRIDGEPORT, PENNSYLVANIA ( Rear platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>19_israel_iran_and_to</td>\n",
       "      <td>[israel, iran, and, to, the, of, israels, that...</td>\n",
       "      <td>[Thank you so much for your kind introduction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>20_to_and_the_we</td>\n",
       "      <td>[to, and, the, we, in, you, of, for, that, is]</td>\n",
       "      <td>[The President. Thank you. Thank you. Audience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>21_the_and_of_to</td>\n",
       "      <td>[the, and, of, to, in, that, we, for, this, they]</td>\n",
       "      <td>[Good evening my fellow Americans. Well, as yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "      <td>22_2008_john_mccain_pm</td>\n",
       "      <td>[2008, john, mccain, pm, arlington, edt, tuesd...</td>\n",
       "      <td>[ARLINGTON, VA -- U.S. Senator John McCain's p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>23_and_king_of_dr</td>\n",
       "      <td>[and, king, of, dr, the, to, we, that, in, was]</td>\n",
       "      <td>[Thank you. Alvieda King, Ralph Abernathy Jr.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>24_health_care_insurance_and</td>\n",
       "      <td>[health, care, insurance, and, of, cancer, med...</td>\n",
       "      <td>[Thank you for the opportunity to talk today a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>25_the_of_and_to</td>\n",
       "      <td>[the, of, and, to, in, that, have, we, you, ohio]</td>\n",
       "      <td>[My fellow citizens: It gives me a great deal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>31</td>\n",
       "      <td>26_that_church_and_to</td>\n",
       "      <td>[that, church, and, to, of, the, in, you, my, is]</td>\n",
       "      <td>[It's great to be here. I've been speaking to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>27_of_the_and_in</td>\n",
       "      <td>[of, the, and, in, to, which, our, is, that, be]</td>\n",
       "      <td>[Mr. Chairman, Members of the Notification Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28_you_to_and_the</td>\n",
       "      <td>[you, to, and, the, tennessee, we, in, that, o...</td>\n",
       "      <td>[Governor Clement, ladies and gentlemen, boys ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>29_the_in_of_this</td>\n",
       "      <td>[the, in, of, this, and, is, illinois, that, a...</td>\n",
       "      <td>[SENATOR KENNEDY. I want to thank Judge Kerner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>30_of_and_our_the</td>\n",
       "      <td>[of, and, our, the, to, that, in, we, for, is]</td>\n",
       "      <td>[ANNOUNCEMENT: Eleven o'clock, election eve, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>31_and_to_the_for</td>\n",
       "      <td>[and, to, the, for, in, of, that, you, we, so]</td>\n",
       "      <td>[Thank you, Jane, for that kind introduction. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>32_applause_dole_going_dolekemp</td>\n",
       "      <td>[applause, dole, going, dolekemp, and, you, to...</td>\n",
       "      <td>[Thank you. [applause] AUDIENCE: Bob Dole! Bob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>33_the_to_of_and</td>\n",
       "      <td>[the, to, of, and, in, you, that, wisconsin, i...</td>\n",
       "      <td>[Governor, Mayor, Senators, members of the del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>34_and_the_to_iowa</td>\n",
       "      <td>[and, the, to, iowa, of, in, you, we, for, our]</td>\n",
       "      <td>[The President. Thank you very much. It's good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>35_the_that_of_in</td>\n",
       "      <td>[the, that, of, in, to, and, have, you, kansas...</td>\n",
       "      <td>[Bob, Larry Winn, Garner Shriver, Keith Sebeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>36_think_we_the_in</td>\n",
       "      <td>[think, we, the, in, is, this, applause, calif...</td>\n",
       "      <td>[Senator KENNEDY. Thank you, Governor. Governo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>37_teachers_education_to_and</td>\n",
       "      <td>[teachers, education, to, and, our, school, th...</td>\n",
       "      <td>[It is an honor to be here with all of you tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "      <td>38_and_the_to_that</td>\n",
       "      <td>[and, the, to, that, you, it, of, but, this, for]</td>\n",
       "      <td>[Congressman Kleppe, Congressman Andrews, Mr. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>39_and_the_we_to</td>\n",
       "      <td>[and, the, we, to, you, that, in, of, californ...</td>\n",
       "      <td>[The President. Thank you. And thank you, Gove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>40_georgia_russian_russia_georgian</td>\n",
       "      <td>[georgia, russian, russia, georgian, ossetia, ...</td>\n",
       "      <td>[Good morning. The situation in Georgia contin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "      <td>41_virginia_the_to_of</td>\n",
       "      <td>[virginia, the, to, of, and, west, that, you, ...</td>\n",
       "      <td>[[1.] CLARKSBURG, WEST VIRGINIA (9 a.m.) Thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>42_applause_and_to_going</td>\n",
       "      <td>[applause, and, to, going, you, were, she, our...</td>\n",
       "      <td>[Thank you. Let's give them another hand. They...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>43_ginsburg_her_arlington_statement</td>\n",
       "      <td>[ginsburg, her, arlington, statement, va, issu...</td>\n",
       "      <td>[ARLINGTON, VA -- Today, U.S. Senator John McC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>44</td>\n",
       "      <td>15</td>\n",
       "      <td>44_indiana_the_that_in</td>\n",
       "      <td>[indiana, the, that, in, is, this, of, you, to...</td>\n",
       "      <td>[Senator KENNEDY. Thank you, Senator Welsh, Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>45_minnesota_the_to_in</td>\n",
       "      <td>[minnesota, the, to, in, and, of, that, you, i...</td>\n",
       "      <td>[Senator Anderson, Governor Perpich, Mayor Sme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>46</td>\n",
       "      <td>15</td>\n",
       "      <td>46_nuclear_north_korea_zimbabwe</td>\n",
       "      <td>[nuclear, north, korea, zimbabwe, weapons, kor...</td>\n",
       "      <td>[ARLINGTON, VA -- U.S. Senator John McCain tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>47_the_to_we_and</td>\n",
       "      <td>[the, to, we, and, of, in, have, that, you, me]</td>\n",
       "      <td>[Thank you very, very much, my very good frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "      <td>48_would_carter_governor_the</td>\n",
       "      <td>[would, carter, governor, the, be, of, that, t...</td>\n",
       "      <td>[Q. Governor, what would you do to deal with i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>49_kentucky_you_the_and</td>\n",
       "      <td>[kentucky, you, the, and, to, that, in, of, ha...</td>\n",
       "      <td>[The President. Thank you, Senator. Thank you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>50_the_of_in_that</td>\n",
       "      <td>[the, of, in, that, have, and, to, this, they,...</td>\n",
       "      <td>[Now, if I might, I would like to give you a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>51_the_ohio_in_of</td>\n",
       "      <td>[the, ohio, in, of, is, this, applause, and, t...</td>\n",
       "      <td>[Senator KENNEDY. Governor Di Salle, distingui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>52_jersey_new_that_to</td>\n",
       "      <td>[jersey, new, that, to, and, in, the, you, we,...</td>\n",
       "      <td>[Thank you very much. Thank you, in the back, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>53_social_older_security_retirement</td>\n",
       "      <td>[social, older, security, retirement, for, ame...</td>\n",
       "      <td>[Good afternoon: A President signs many bills,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>54_virus_and_to_trump</td>\n",
       "      <td>[virus, and, to, trump, this, will, the, healt...</td>\n",
       "      <td>[As prepared for delivery.\\n Today, America wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>55_the_you_to_and</td>\n",
       "      <td>[the, you, to, and, clinton, governor, it, goi...</td>\n",
       "      <td>[The President. Thank you very, very much. Tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>56_that_the_this_in</td>\n",
       "      <td>[that, the, this, in, have, it, is, we, and, you]</td>\n",
       "      <td>[Thank you, Senator Hughes. You know, I wish s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                 Name  \\\n",
       "0      -1   1173                     -1_the_to_and_of   \n",
       "1       0    240                 0_the_in_applause_of   \n",
       "2       1    111                      1_to_you_and_we   \n",
       "3       2     95                      2_the_of_our_in   \n",
       "4       3     80                     3_the_to_and_our   \n",
       "5       4     77                4_you_clinton_to_that   \n",
       "6       5     76                     5_of_the_farm_in   \n",
       "7       6     75                     6_and_our_the_to   \n",
       "8       7     70                     7_of_the_in_that   \n",
       "9       8     61                   8_iraq_that_the_to   \n",
       "10      9     55                   9_the_texas_in_and   \n",
       "11     10     55         10_crisis_financial_that_and   \n",
       "12     11     55                    11_that_the_in_of   \n",
       "13     12     54               12_the_and_carolina_to   \n",
       "14     13     52                  13_and_to_jobs_will   \n",
       "15     14     39                 14_energy_oil_of_and   \n",
       "16     15     38        15_mccain_obama_john_economic   \n",
       "17     16     37                     16_to_and_you_we   \n",
       "18     17     37                   17_to_and_the_will   \n",
       "19     18     36                    18_to_the_you_and   \n",
       "20     19     35                19_israel_iran_and_to   \n",
       "21     20     34                     20_to_and_the_we   \n",
       "22     21     34                     21_the_and_of_to   \n",
       "23     22     34               22_2008_john_mccain_pm   \n",
       "24     23     33                    23_and_king_of_dr   \n",
       "25     24     31         24_health_care_insurance_and   \n",
       "26     25     31                     25_the_of_and_to   \n",
       "27     26     31                26_that_church_and_to   \n",
       "28     27     29                     27_of_the_and_in   \n",
       "29     28     28                    28_you_to_and_the   \n",
       "30     29     27                    29_the_in_of_this   \n",
       "31     30     27                    30_of_and_our_the   \n",
       "32     31     27                    31_and_to_the_for   \n",
       "33     32     24      32_applause_dole_going_dolekemp   \n",
       "34     33     24                     33_the_to_of_and   \n",
       "35     34     24                   34_and_the_to_iowa   \n",
       "36     35     22                    35_the_that_of_in   \n",
       "37     36     21                   36_think_we_the_in   \n",
       "38     37     21         37_teachers_education_to_and   \n",
       "39     38     20                   38_and_the_to_that   \n",
       "40     39     20                     39_and_the_we_to   \n",
       "41     40     20   40_georgia_russian_russia_georgian   \n",
       "42     41     20                41_virginia_the_to_of   \n",
       "43     42     17             42_applause_and_to_going   \n",
       "44     43     16  43_ginsburg_her_arlington_statement   \n",
       "45     44     15               44_indiana_the_that_in   \n",
       "46     45     15               45_minnesota_the_to_in   \n",
       "47     46     15      46_nuclear_north_korea_zimbabwe   \n",
       "48     47     14                     47_the_to_we_and   \n",
       "49     48     14         48_would_carter_governor_the   \n",
       "50     49     14              49_kentucky_you_the_and   \n",
       "51     50     13                    50_the_of_in_that   \n",
       "52     51     13                    51_the_ohio_in_of   \n",
       "53     52     13                52_jersey_new_that_to   \n",
       "54     53     13  53_social_older_security_retirement   \n",
       "55     54     13                54_virus_and_to_trump   \n",
       "56     55     12                    55_the_you_to_and   \n",
       "57     56     11                  56_that_the_this_in   \n",
       "\n",
       "                                       Representation  \\\n",
       "0      [the, to, and, of, in, that, we, you, for, is]   \n",
       "1   [the, in, applause, of, is, and, that, we, thi...   \n",
       "2    [to, you, and, we, the, in, that, for, of, have]   \n",
       "3       [the, of, our, in, and, to, that, we, is, as]   \n",
       "4        [the, to, and, our, for, he, in, of, is, we]   \n",
       "5   [you, clinton, to, that, and, the, know, mccai...   \n",
       "6   [of, the, farm, in, and, to, that, is, it, far...   \n",
       "7    [and, our, the, to, of, we, in, iraq, that, war]   \n",
       "8   [of, the, in, that, to, and, would, have, is, be]   \n",
       "9   [iraq, that, the, to, mccain, troops, and, we,...   \n",
       "10  [the, texas, in, and, of, you, that, to, have,...   \n",
       "11  [crisis, financial, that, and, street, mccain,...   \n",
       "12     [that, the, in, of, we, is, this, and, it, to]   \n",
       "13  [the, and, carolina, to, of, in, that, you, we...   \n",
       "14  [and, to, jobs, will, the, that, of, tax, we, ...   \n",
       "15  [energy, oil, of, and, the, our, we, to, that,...   \n",
       "16  [mccain, obama, john, economic, jobs, senator,...   \n",
       "17  [to, and, you, we, michigan, the, in, for, tha...   \n",
       "18  [to, and, the, will, tax, we, for, in, going, of]   \n",
       "19     [to, the, you, and, in, of, is, for, we, that]   \n",
       "20  [israel, iran, and, to, the, of, israels, that...   \n",
       "21     [to, and, the, we, in, you, of, for, that, is]   \n",
       "22  [the, and, of, to, in, that, we, for, this, they]   \n",
       "23  [2008, john, mccain, pm, arlington, edt, tuesd...   \n",
       "24    [and, king, of, dr, the, to, we, that, in, was]   \n",
       "25  [health, care, insurance, and, of, cancer, med...   \n",
       "26  [the, of, and, to, in, that, have, we, you, ohio]   \n",
       "27  [that, church, and, to, of, the, in, you, my, is]   \n",
       "28   [of, the, and, in, to, which, our, is, that, be]   \n",
       "29  [you, to, and, the, tennessee, we, in, that, o...   \n",
       "30  [the, in, of, this, and, is, illinois, that, a...   \n",
       "31     [of, and, our, the, to, that, in, we, for, is]   \n",
       "32     [and, to, the, for, in, of, that, you, we, so]   \n",
       "33  [applause, dole, going, dolekemp, and, you, to...   \n",
       "34  [the, to, of, and, in, you, that, wisconsin, i...   \n",
       "35    [and, the, to, iowa, of, in, you, we, for, our]   \n",
       "36  [the, that, of, in, to, and, have, you, kansas...   \n",
       "37  [think, we, the, in, is, this, applause, calif...   \n",
       "38  [teachers, education, to, and, our, school, th...   \n",
       "39  [and, the, to, that, you, it, of, but, this, for]   \n",
       "40  [and, the, we, to, you, that, in, of, californ...   \n",
       "41  [georgia, russian, russia, georgian, ossetia, ...   \n",
       "42  [virginia, the, to, of, and, west, that, you, ...   \n",
       "43  [applause, and, to, going, you, were, she, our...   \n",
       "44  [ginsburg, her, arlington, statement, va, issu...   \n",
       "45  [indiana, the, that, in, is, this, of, you, to...   \n",
       "46  [minnesota, the, to, in, and, of, that, you, i...   \n",
       "47  [nuclear, north, korea, zimbabwe, weapons, kor...   \n",
       "48    [the, to, we, and, of, in, have, that, you, me]   \n",
       "49  [would, carter, governor, the, be, of, that, t...   \n",
       "50  [kentucky, you, the, and, to, that, in, of, ha...   \n",
       "51  [the, of, in, that, have, and, to, this, they,...   \n",
       "52  [the, ohio, in, of, is, this, applause, and, t...   \n",
       "53  [jersey, new, that, to, and, in, the, you, we,...   \n",
       "54  [social, older, security, retirement, for, ame...   \n",
       "55  [virus, and, to, trump, this, will, the, healt...   \n",
       "56  [the, you, to, and, clinton, governor, it, goi...   \n",
       "57  [that, the, this, in, have, it, is, we, and, you]   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [Thank you very much for being here and introd...  \n",
       "1   [Senator KENNEDY. Lieutenant Governor Swainson...  \n",
       "2   [The President. Thank you. Audience members. F...  \n",
       "3   [For the past seventeen months, as a candidate...  \n",
       "4   [The President. Thank you all for coming today...  \n",
       "5   [CLINTON: Hello, Unity! Hello, New Hampshire! ...  \n",
       "6   [Senator KENNEDY. My friend and colleague, Con...  \n",
       "7   [Thank you, Commander Kurpius, for that introd...  \n",
       "8   [The ceremony of an acceptance speech is a tra...  \n",
       "9   [A few months ago, I met a woman who told me h...  \n",
       "10  [Senator Tower, Congressman Bush, all of the d...  \n",
       "11  [We meet here at a time of great uncertainty f...  \n",
       "12  [Gov. JOHN F. DAVIS. North Dakota is again hon...  \n",
       "13  [I realize that in this great audience are man...  \n",
       "14  [We meet at a moment of great uncertainty for ...  \n",
       "15  [Two weeks ago, representatives from some of t...  \n",
       "16  [NORRIS: Senator Barack Obama was in St. Louis...  \n",
       "17  [Hello Detroit! Thank you! I'll tell you, I am...  \n",
       "18  [Thank you Arnold! Thank you Ohio! We need to ...  \n",
       "19  [[1.] BRIDGEPORT, PENNSYLVANIA ( Rear platform...  \n",
       "20  [Thank you so much for your kind introduction ...  \n",
       "21  [The President. Thank you. Thank you. Audience...  \n",
       "22  [Good evening my fellow Americans. Well, as yo...  \n",
       "23  [ARLINGTON, VA -- U.S. Senator John McCain's p...  \n",
       "24  [Thank you. Alvieda King, Ralph Abernathy Jr.,...  \n",
       "25  [Thank you for the opportunity to talk today a...  \n",
       "26  [My fellow citizens: It gives me a great deal ...  \n",
       "27  [It's great to be here. I've been speaking to ...  \n",
       "28  [Mr. Chairman, Members of the Notification Com...  \n",
       "29  [Governor Clement, ladies and gentlemen, boys ...  \n",
       "30  [SENATOR KENNEDY. I want to thank Judge Kerner...  \n",
       "31  [ANNOUNCEMENT: Eleven o'clock, election eve, 1...  \n",
       "32  [Thank you, Jane, for that kind introduction. ...  \n",
       "33  [Thank you. [applause] AUDIENCE: Bob Dole! Bob...  \n",
       "34  [Governor, Mayor, Senators, members of the del...  \n",
       "35  [The President. Thank you very much. It's good...  \n",
       "36  [Bob, Larry Winn, Garner Shriver, Keith Sebeli...  \n",
       "37  [Senator KENNEDY. Thank you, Governor. Governo...  \n",
       "38  [It is an honor to be here with all of you tod...  \n",
       "39  [Congressman Kleppe, Congressman Andrews, Mr. ...  \n",
       "40  [The President. Thank you. And thank you, Gove...  \n",
       "41  [Good morning. The situation in Georgia contin...  \n",
       "42  [[1.] CLARKSBURG, WEST VIRGINIA (9 a.m.) Thank...  \n",
       "43  [Thank you. Let's give them another hand. They...  \n",
       "44  [ARLINGTON, VA -- Today, U.S. Senator John McC...  \n",
       "45  [Senator KENNEDY. Thank you, Senator Welsh, Se...  \n",
       "46  [Senator Anderson, Governor Perpich, Mayor Sme...  \n",
       "47  [ARLINGTON, VA -- U.S. Senator John McCain tod...  \n",
       "48  [Thank you very, very much, my very good frien...  \n",
       "49  [Q. Governor, what would you do to deal with i...  \n",
       "50  [The President. Thank you, Senator. Thank you,...  \n",
       "51  [Now, if I might, I would like to give you a r...  \n",
       "52  [Senator KENNEDY. Governor Di Salle, distingui...  \n",
       "53  [Thank you very much. Thank you, in the back, ...  \n",
       "54  [Good afternoon: A President signs many bills,...  \n",
       "55  [As prepared for delivery.\\n Today, America wi...  \n",
       "56  [The President. Thank you very, very much. Tha...  \n",
       "57  [Thank you, Senator Hughes. You know, I wish s...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_refined = dict()\n",
    "topics_refined[\"environment\"] = [18]\n",
    "topics_refined[\"economy\"] = [3,11,13,43,45]\n",
    "topics_refined[\"diplomacy\"] = [31,56]\n",
    "topics_refined[\"immigration\"] = [29]\n",
    "topics_refined[\"justice\"] = [12,58]\n",
    "topics_refined[\"education\"] = [42]\n",
    "topics_refined[\"Healthcare\"] = [40,52,59]\n",
    "topics_refined[\"military\"] = [5,7]\n",
    "topics_refined[\"ideal\"] = [1,21,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(df_all[\"speech_whole\"].apply(filter_externals))\n",
    "\n",
    "for topic, columns in topics_refined.items():\n",
    "  print(topic_distr[:,columns])\n",
    "  df_all[f\"{topic}_importance\"] = topic_distr[:,columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(df_2024[\"speech_whole\"].apply(filter_externals))\n",
    "\n",
    "for topic, columns in topics_refined.items():\n",
    "  print(topic_distr[:,columns])\n",
    "  df_2024[f\"{topic}_importance\"] = topic_distr[:,columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(path+'text_features.csv')\n",
    "df_2024.to_csv(path+'2024_text_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation: LDA 기반 중요도 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"../data/debate_combined_ver2.csv\", index_col=0)\n",
    "df_2024 = pd.read_csv(\"../data/2024_combined.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all[\"speech_processed2\"] = df_all[\"speech_processed2\"].apply(ast.literal_eval)\n",
    "# df_2024[\"speech_processed2\"] = df_2024[\"speech_processed2\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install gensim -y\n",
    "\n",
    "dictionary = corpora.Dictionary(df_all[\"speech_processed2\"].apply(ast.literal_eval))\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in df_all[\"speech_processed2\"].apply(ast.literal_eval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.010*\"peace\" + 0.009*\"must\" + 0.008*\"things\" + 0.007*\"friends\" + 0.006*\"war\" + 0.006*\"freedom\" + 0.006*\"tell\" + 0.006*\"thats\" + 0.005*\"progress\" + 0.005*\"strength\"')\n",
      "(1, '0.014*\"energy\" + 0.012*\"health\" + 0.010*\"care\" + 0.010*\"oil\" + 0.008*\"jobs\" + 0.006*\"economy\" + 0.006*\"workers\" + 0.005*\"plan\" + 0.004*\"costs\" + 0.004*\"families\"')\n",
      "(2, '0.010*\"thats\" + 0.009*\"im\" + 0.006*\"got\" + 0.006*\"theyre\" + 0.005*\"trump\" + 0.005*\"see\" + 0.005*\"really\" + 0.005*\"lot\" + 0.005*\"care\" + 0.004*\"look\"')\n",
      "(3, '0.008*\"federal\" + 0.007*\"must\" + 0.005*\"economic\" + 0.005*\"national\" + 0.005*\"public\" + 0.005*\"business\" + 0.005*\"upon\" + 0.004*\"administration\" + 0.004*\"system\" + 0.004*\"nation\"')\n",
      "(4, '0.011*\"let\" + 0.010*\"congress\" + 0.009*\"next\" + 0.008*\"inflation\" + 0.007*\"2\" + 0.006*\"percent\" + 0.006*\"peace\" + 0.006*\"months\" + 0.006*\"tax\" + 0.005*\"4\"')\n",
      "(5, '0.014*\"war\" + 0.013*\"must\" + 0.006*\"iraq\" + 0.005*\"security\" + 0.005*\"military\" + 0.004*\"never\" + 0.003*\"nation\" + 0.003*\"veterans\" + 0.003*\"americas\" + 0.003*\"fight\"')\n",
      "(6, '0.011*\"education\" + 0.011*\"children\" + 0.008*\"school\" + 0.008*\"college\" + 0.008*\"4\" + 0.007*\"million\" + 0.007*\"century\" + 0.007*\"build\" + 0.006*\"give\" + 0.006*\"future\"')\n",
      "(7, '0.006*\"must\" + 0.006*\"rights\" + 0.004*\"law\" + 0.004*\"justice\" + 0.004*\"children\" + 0.004*\"life\" + 0.004*\"community\" + 0.004*\"nation\" + 0.003*\"faith\" + 0.003*\"public\"')\n",
      "(8, '0.015*\"tax\" + 0.012*\"jobs\" + 0.011*\"thats\" + 0.010*\"economy\" + 0.008*\"plan\" + 0.007*\"taxes\" + 0.007*\"health\" + 0.007*\"senator\" + 0.006*\"washington\" + 0.006*\"care\"')\n",
      "(9, '0.020*\"governor\" + 0.015*\"carter\" + 0.010*\"q\" + 0.007*\"percent\" + 0.005*\"federal\" + 0.005*\"congress\" + 0.005*\"ought\" + 0.005*\"tax\" + 0.004*\"georgia\" + 0.004*\"unemployment\"')\n",
      "(10, '0.011*\"man\" + 0.010*\"peace\" + 0.010*\"best\" + 0.009*\"senate\" + 0.009*\"love\" + 0.009*\"john\" + 0.008*\"see\" + 0.007*\"governor\" + 0.006*\"somebody\" + 0.006*\"working\"')\n",
      "(11, '0.014*\"mr\" + 0.009*\"party\" + 0.008*\"senator\" + 0.006*\"nixon\" + 0.006*\"kennedy\" + 0.006*\"freedom\" + 0.005*\"must\" + 0.005*\"democratic\" + 0.005*\"question\" + 0.004*\"next\"')\n",
      "(12, '0.017*\"republican\" + 0.016*\"democratic\" + 0.014*\"congress\" + 0.011*\"party\" + 0.011*\"farm\" + 0.009*\"farmers\" + 0.009*\"republicans\" + 0.008*\"vote\" + 0.006*\"program\" + 0.005*\"farmer\"')\n",
      "(13, '0.015*\"peace\" + 0.011*\"man\" + 0.010*\"nation\" + 0.009*\"tonight\" + 0.008*\"let\" + 0.007*\"men\" + 0.007*\"war\" + 0.006*\"young\" + 0.006*\"must\" + 0.005*\"friends\"')\n",
      "(14, '0.022*\"clinton\" + 0.018*\"hillary\" + 0.011*\"jobs\" + 0.007*\"trade\" + 0.006*\"crime\" + 0.005*\"percent\" + 0.004*\"immigration\" + 0.004*\"illegal\" + 0.004*\"ever\" + 0.004*\"law\"')\n",
      "(15, '0.009*\"im\" + 0.007*\"got\" + 0.007*\"health\" + 0.007*\"members\" + 0.006*\"weve\" + 0.006*\"audience\" + 0.005*\"care\" + 0.005*\"sure\" + 0.005*\"security\" + 0.005*\"hes\"')\n",
      "(16, '0.015*\"4\" + 0.010*\"nation\" + 0.008*\"audience\" + 0.006*\"reagan\" + 0.006*\"weve\" + 0.006*\"im\" + 0.006*\"future\" + 0.005*\"tax\" + 0.005*\"inflation\" + 0.005*\"ive\"')\n",
      "(17, '0.010*\"got\" + 0.010*\"clinton\" + 0.009*\"im\" + 0.009*\"governor\" + 0.009*\"dole\" + 0.009*\"tax\" + 0.008*\"let\" + 0.008*\"thats\" + 0.007*\"hes\" + 0.007*\"weve\"')\n",
      "(18, '0.024*\"mccain\" + 0.018*\"senator\" + 0.011*\"im\" + 0.009*\"clinton\" + 0.008*\"campaign\" + 0.007*\"lot\" + 0.006*\"sen\" + 0.006*\"thats\" + 0.005*\"got\" + 0.005*\"look\"')\n",
      "(19, '0.013*\"iraq\" + 0.009*\"nuclear\" + 0.007*\"iran\" + 0.007*\"israel\" + 0.006*\"troops\" + 0.006*\"security\" + 0.006*\"military\" + 0.006*\"weapons\" + 0.005*\"war\" + 0.005*\"got\"')\n"
     ]
    }
   ],
   "source": [
    "n_topics = 20\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=n_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topics = lda_model.print_topics(num_words=2)\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = list(dictionary.values())\n",
    "# words.sort(key=lambda x:len(x), reverse=True)\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_refined = dict()\n",
    "# topics_refined[\"environment\"] = []\n",
    "topics_refined[\"economy\"] = [1,3,4,8]\n",
    "# topics_refined[\"diplomacy\"] = []\n",
    "# topics_refined[\"immigration\"] = []\n",
    "topics_refined[\"justice\"] = [7]\n",
    "topics_refined[\"education\"] = [6]\n",
    "# topics_refined[\"Healthcare\"] = []\n",
    "topics_refined[\"military\"] = [5,19]\n",
    "topics_refined[\"ideal\"] = [0,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = lda_model.inference(corpus)\n",
    "topic_distr,_ = inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic, columns in topics_refined.items():\n",
    "  # print(topic_distr[:,columns])\n",
    "  df_all[f\"{topic}_importance\"] = topic_distr[:,columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_2024 = corpora.Dictionary(df_2024[\"speech_processed2\"].apply(ast.literal_eval))\n",
    "\n",
    "corpus_2024 = [dictionary.doc2bow(text) for text in df_2024[\"speech_processed2\"].apply(ast.literal_eval)\n",
    "]\n",
    "inference_2024 = lda_model.inference(corpus_2024)\n",
    "topic_distr_2024,_ = inference_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic, columns in topics_refined.items():\n",
    "  # print(topic_distr[:,columns])\n",
    "  df_2024[f\"{topic}_importance\"] = topic_distr_2024[:,columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.drop(\"speech_processed\",axis=1)\n",
    "df_2024 = df_2024.drop(\"speech_processed\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('./data/text_features_LDA.csv')\n",
    "df_2024.to_csv('./data/2024_text_features_LDA.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
