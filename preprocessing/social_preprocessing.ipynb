{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import ast\n",
    "import nltk\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시드 고정\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data path\n",
    "path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_df = pd.read_csv(path+'candidate_merged_Data_v3.csv',index_col=0)\n",
    "social_df = social_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_coord = {\"New York\": (40.7128, 74.0060),\n",
    " \"Texas\": (31.9686, 99.9018),\n",
    " \"Massachusetts\": (42.4072,71.3824),\n",
    " \"California\": (36.7783, 119.4179),\n",
    " \"Illinois\": (40.6331, 89.3985),\n",
    " \"Arkansas\": (35.2010, 91.8318),\n",
    " \"Hawaii\": (19.8987, 155.6659),\n",
    " \"Pennsylvania\": (41.2033, 77.1945),\n",
    " \"Virginia\": (37.4316, 78.6569),\n",
    " \"Ohio\": (40.4173, 82.9071),\n",
    " \"Vermont\": (44.5588, 72.5778),\n",
    " \"Iowa\": (41.8780, 93.0977),\n",
    " \"Missouri\": (37.9643, 91.8318),\n",
    " \"Georgia\": (32.1574, 82.9071),\n",
    " \"Kansas\": (39.0119, 98.4842),\n",
    " \"Arizona\": (34.0489,111.0937),\n",
    " \"South Dakota\": (43.9695, 99.9018),\n",
    " \"Michigan\": (44.3148, 85.6024),\n",
    " \"West Virginia\": (38.5976, 80.4549),\n",
    " \"Indiana\": (40.5512, 85.6024),\n",
    " \"Minnesota\": (46.7296, 94.6859),\n",
    " \"Tennessee\": (35.5175, 86.5804)\n",
    " }\n",
    "\n",
    "#region feature\n",
    "social_df[\"region_x\"] = social_df[\"Region\"].apply(lambda x: region_coord[x][0] if pd.notna(x) else x)\n",
    "social_df[\"region_y\"] = social_df[\"Region\"].apply(lambda x: region_coord[x][1] if pd.notna(x) else x)\n",
    "\n",
    "social_df[\"c_region_x\"] = social_df[\"c_Region\"].apply(lambda x: region_coord[x][0] if pd.notna(x) else x)\n",
    "social_df[\"c_region_y\"] = social_df[\"c_Region\"].apply(lambda x: region_coord[x][1] if pd.notna(x) else x)\n",
    "\n",
    "social_df = social_df.drop(columns=[\"Region\", \"c_Region\"])\n",
    "\n",
    "#education\n",
    "education = {\"X\": 0, \"Bachelor's\": 1, \"JD\": 2, \"Master's\": 3, \"PhD\": 4}\n",
    "social_df[\"Final Education\"] = social_df[\"Final Education\"].apply(lambda x: education[x] if pd.notna(x) else x)\n",
    "social_df[\"c_Final_Education\"] = social_df[\"c_Final_Education\"].apply(lambda x: education[x] if pd.notna(x) else x)\n",
    "\n",
    "#party\n",
    "social_df[\"Party\"] = social_df[\"Party\"].apply(lambda x: (1 if x == \"Republican\" else 0) if pd.notna(x) else x)\n",
    "social_df[\"c_Party\"] = social_df[\"c_Party\"].apply(lambda x: (1 if x == \"Republican\" else 0) if pd.notna(x) else x)\n",
    "\n",
    "\n",
    "pres_df = social_df.drop([\"c_Final_Education\", \"c_region_x\",\"c_region_y\", \"c_Party\",\"c_Age\"],axis=1)\n",
    "cand_df = social_df.drop([\"Final Education\",\"region_x\",\"region_y\", \"Party\",\"Age\"],axis=1)\n",
    "cand_df = cand_df.rename(columns={\"c_Final_Education\": \"Final Education\", \"c_region_x\": \"region_x\", \"c_region_y\": \"region_y\", \"c_Party\": \"Party\", \"c_Age\": \"Age\"})\n",
    "\n",
    "\n",
    "# party_republican = old data for old model, replace with candidate elected\n",
    "pres_df[\"elected\"] = (pres_df[\"Party_Republican\"] == pres_df[\"Party\"]).apply(lambda x: 1 if x else 0)\n",
    "cand_df[\"elected\"] = (cand_df[\"Party_Republican\"] == cand_df[\"Party\"]).apply(lambda x: 1 if x else 0)\n",
    "\n",
    "pres_df = pres_df.drop(columns=\"Party_Republican\")\n",
    "cand_df = cand_df.drop(columns=\"Party_Republican\")\n",
    "\n",
    "\n",
    "#split train and test\n",
    "social_train_df = pd.concat([pres_df.iloc[:-4],cand_df.iloc[:-4]])\n",
    "social_test_df = pd.concat([pres_df.iloc[-4:], cand_df.iloc[-4:]])\n",
    "\n",
    "#select random 5 instance of 4-years\n",
    "valid_years = [1960,\n",
    "1952,\n",
    "1968,\n",
    "1984,\n",
    "2012]\n",
    "\n",
    "social_valid_df = pd.concat([social_train_df[social_train_df[\"Year\"].isin([1949,1950,1951,1952])],\n",
    "                      social_train_df[social_train_df[\"Year\"].isin([1957,1958,1959,1960])],\n",
    "                      social_train_df[social_train_df[\"Year\"].isin([1965,1966,1967,1968])],\n",
    "                      social_train_df[social_train_df[\"Year\"].isin([1981,1982,1983,1984])],\n",
    "                      social_train_df[social_train_df[\"Year\"].isin([2009,2010,2011,2012])]\n",
    "                      ])\n",
    "\n",
    "social_train_df = social_train_df[~social_train_df[\"Year\"].isin([1949,1950,1951,1952,1957,1958,1959,1960,1965,1966,1967,1968,1981,1982,1983,1984,2009,2010,2011,2012])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale\n",
    "\n",
    "social_scaler = MinMaxScaler()\n",
    "\n",
    "to_scale = [\"Approval_Rating\", \"CPI\", \"GDP\", \"Employment status\", \"Final Education\", \"Age\", \"region_x\", \"region_y\"]\n",
    "\n",
    "social_train_df[to_scale] = social_scaler.fit_transform(social_train_df[to_scale])\n",
    "social_valid_df[to_scale] = social_scaler.transform(social_valid_df[to_scale])\n",
    "social_test_df[to_scale] = social_scaler.transform(social_test_df[to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_xy(df):\n",
    "\n",
    "  social_x = df.drop(\"elected\",axis=1)\n",
    "  social_y = df[\"elected\"]\n",
    "\n",
    "  combined_4year_x = list()\n",
    "  for i in range(0,social_x.values.shape[0],4):\n",
    "    combined_4year_x.append(social_x.values[i:i+4])\n",
    "\n",
    "  combined_4year_y = list()\n",
    "  for i in range(0,social_y.values.shape[0],4):\n",
    "    combined_4year_y.append(social_y.values[i])\n",
    "\n",
    "  combined_4year_x = np.array(combined_4year_x)\n",
    "  combined_4year_y = np.array(combined_4year_y)\n",
    "\n",
    "\n",
    "\n",
    "  return combined_4year_x, combined_4year_y\n",
    "\n",
    "\n",
    "social_train_x, social_train_y = divide_xy(social_train_df)\n",
    "social_valid_x, social_valid_y = divide_xy(social_valid_df)\n",
    "social_test_x, social_test_y = divide_xy(social_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data\n",
    "\n",
    "# with open(path+'ind_train_x.pkl','wb') as f:\n",
    "#   pickle.dump(social_train_x,f)\n",
    "\n",
    "# with open(path+'ind_train_y.pkl','wb') as f:\n",
    "#   pickle.dump(social_train_y,f)\n",
    "\n",
    "# with open(path+'ind_valid_x.pkl','wb') as f:\n",
    "#   pickle.dump(social_valid_x,f)\n",
    "\n",
    "# with open(path+'ind_valid_y.pkl','wb') as f:\n",
    "#   pickle.dump(social_valid_y,f)\n",
    "\n",
    "# with open(path+'ind_test_x.pkl','wb') as f:\n",
    "#   pickle.dump(social_test_x,f)\n",
    "\n",
    "# with open(path+'ind_test_y.pkl','wb') as f:\n",
    "#   pickle.dump(social_test_y,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_filename = 'text_features_LDA.csv'\n",
    "orig_filename_2024 = '2024_text_features_LDA.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv(path+orig_filename, index_col=0)\n",
    "\n",
    "recent_df = pd.read_csv(path+orig_filename_2024, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_year(df):\n",
    "  df[\"speech_year\"] = df[\"date\"].apply(lambda x: x.rstrip()[-4:])\n",
    "  return\n",
    "\n",
    "add_year(text_df)\n",
    "add_year(recent_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_df[\"speech_year\"] = text_df[\"speech_year\"].astype(np.int64)\n",
    "recent_df[\"speech_year\"] = recent_df[\"speech_year\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = text_df[\"speech_processed2\"].apply(lambda x:ast.literal_eval(x))\n",
    "lengths = lengths.reset_index()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "lengths[\"speech_processed2\"].hist(ax=ax)\n",
    "ax.set(xlabel='Word count', ylabel='Documents', title='Document Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"speech_divide_to_10\"] = text_df[\"speech_processed2\"].apply(lambda x: ast.literal_eval(x)).apply(lambda x: [x[i:min(len(x),i+100)] for i in range(0,len(x),100)])\n",
    "recent_df[\"speech_divide_to_10\"] = recent_df[\"speech_processed2\"].apply(lambda x: ast.literal_eval(x)).apply(lambda x: [x[i:min(len(x),i+100)] for i in range(0,len(x),100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text_df = pd.merge(text_df,social_df,how='left',left_on='speech_year', right_on='Year',suffixes=[None,\"_y\"])\n",
    "merged_test_df = pd.merge(recent_df,social_df,how='left',left_on='speech_year', right_on='Year',suffixes=[None,\"_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text_df[\"speech_divide_to_10\"] = merged_text_df[\"speech_processed2\"].apply(lambda x: ast.literal_eval(x)).apply(lambda x: [x[i:min(len(x),i+100)] for i in range(0,len(x),100)])\n",
    "merged_test_df[\"speech_divide_to_10\"] = merged_test_df[\"speech_processed2\"].apply(lambda x: ast.literal_eval(x)).apply(lambda x: [x[i:min(len(x),i+100)] for i in range(0,len(x),100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_number(name):\n",
    "  if 'Biden'.lower() in name.lower():\n",
    "    return 0\n",
    "  elif 'Trump'.lower() in name.lower():\n",
    "    return 1\n",
    "  else:\n",
    "    return -1\n",
    "merged_test_df[\"elected\"] = merged_test_df[\"name\"].apply(name_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(merged_text_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = merged_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv(path+'processed_'+orig_filename)\n",
    "recent_df.to_csv(path+'processed_'+orig_filename_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(path+'combined_train.csv')\n",
    "valid_df.to_csv(path+'combined_valid.csv')\n",
    "test_df.to_csv(path+'combined_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
